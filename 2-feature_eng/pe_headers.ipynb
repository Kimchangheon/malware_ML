{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():   \n",
    "\n",
    "    #샘플 파일이 위치하는 폴더\n",
    "    source_path= raw_input(\"Enter the path of samples (ending with /) >>  \")\n",
    "    #특징 추출 결과를 저장할 csv파일 이름\n",
    "    output_file= raw_input(\"Give file name of output file. (.csv) >>\")\n",
    "    #label : 카테고리 지정, 악성코드인 경우1, 정상 프로그램은 0\n",
    "    label = raw_input(\"Enter type of sample( malware(1)|benign(0))>>\")\n",
    "\n",
    "    features = pe_features(source_path,output_file,label)    \n",
    "    features.create_dataset()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_dataset함수는 \n",
    "csv파일을 새롭게 생성하고 헤더 이름을 기록 한 후 메인 \n",
    "함수에서 지정한 경로에 있는 파일을 하나씩 가져와 특징을 추출한 후 그 결과를 csv파일에 기록한다. 여기에서 가장 중요한 함수는 바로 extract_all 함수다. 해당 함수는 파일에서 특징을 뽑아내는 핵심 기능을 담고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_dataset(self):\n",
    "        self.write_csv_header() # csv헤더 생성\n",
    "        count = 0\n",
    "\n",
    "        #run through all file of source and extract features\n",
    "        for file in os.listdir(self.source):         #source_path로 지정한 경로의 모든 파일을 가져옴\n",
    "                filepath = self.source + file\n",
    "                data = self.extract_all(filepath) #특징 추출 함수\n",
    "                hash_ = self.getMD5(filepath)\n",
    "                print \"hash: \", hash_\n",
    "                data.insert(0, hash_)\n",
    "                data.insert(0, file)\n",
    "\n",
    "                self.write_csv_data(data)\n",
    "                count += 1\n",
    "                print \"Successfully Data extracted and written for {}.\".format(file)\n",
    "                print \"Processed \" + str(count) + \" files\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 설명한 것처럼 우선 간단하게 PE 헤더 파싱으로 뽑아낼 수 있는 Raw 특징을 추출한다.(extract_dos_header, extract_file_header, extract_optional_header). 다음으로.PE 헤더 요소 ㄱ밧들을 한 번 더 해석해 의미 있는 정보를 추출하는 Derived 특징 추출 부분이 실행된다.\n",
    "Derived특징 추출 함수가 가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1a527f091614>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1a527f091614>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpe_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m     \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1a527f091614>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m                 \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m                 \u001b[0mhash_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMD5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"hash: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1a527f091614>\u001b[0m in \u001b[0;36mextract_all\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;31m# derived features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;31m#number of suspicisou sections and non-suspicsious section\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m             \u001b[0mnum_ss_nss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_count_suspicious_sections\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_ss_nss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[1;31m# check for packer and packer type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1a527f091614>\u001b[0m in \u001b[0;36mget_count_suspicious_sections\u001b[1;34m(self, pe)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mbenign_sections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'.text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.rdata'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.idata'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.edata'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.rsrc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.bss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.crt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.tls'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msection\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msections\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[0mtmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\x00'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[0mnon_sus_sections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbenign_sections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnon_sus_sections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_sus_sections\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "# pip install pefile\n",
    "# pip install yara\n",
    "# find / -name libyara.so \n",
    "# cp LIB_YARA_PATH /home/stud/anaconda2/envs/ml_sec_2.7/lib/\n",
    "\n",
    "import csv,os,pefile\n",
    "import yara\n",
    "import math\n",
    "import hashlib\n",
    "\n",
    "class pe_features():\n",
    "\n",
    "    IMAGE_DOS_HEADER = [\n",
    "                        \"e_cblp\",\\\n",
    "                        \"e_cp\", \\\n",
    "                        \"e_cparhdr\",\\\n",
    "                        \"e_maxalloc\",\\\n",
    "                        \"e_sp\",\\\n",
    "                        \"e_lfanew\"]\n",
    "\n",
    "    FILE_HEADER= [\"NumberOfSections\",\"CreationYear\"] + [ \"FH_char\" + str(i) for i in range(15)]\n",
    "                \n",
    "\n",
    "    OPTIONAL_HEADER1 = [\n",
    "                        \"MajorLinkerVersion\",\\\n",
    "                        \"MinorLinkerVersion\",\\\n",
    "                        \"SizeOfCode\",\\\n",
    "                        \"SizeOfInitializedData\",\\\n",
    "                        \"SizeOfUninitializedData\",\\\n",
    "                        \"AddressOfEntryPoint\",\\\n",
    "                        \"BaseOfCode\",\\\n",
    "                        \"BaseOfData\",\\\n",
    "                        \"ImageBase\",\\\n",
    "                        \"SectionAlignment\",\\\n",
    "                        \"FileAlignment\",\\\n",
    "                        \"MajorOperatingSystemVersion\",\\\n",
    "                        \"MinorOperatingSystemVersion\",\\\n",
    "                        \"MajorImageVersion\",\\\n",
    "                        \"MinorImageVersion\",\\\n",
    "                        \"MajorSubsystemVersion\",\\\n",
    "                        \"MinorSubsystemVersion\",\\\n",
    "                        \"SizeOfImage\",\\\n",
    "                        \"SizeOfHeaders\",\\\n",
    "                        \"CheckSum\",\\\n",
    "                        \"Subsystem\"] \n",
    "    OPTIONAL_HEADER_DLL_char = [ \"OH_DLLchar\" + str(i) for i in range(11)]                   \n",
    "                            \n",
    "    OPTIONAL_HEADER2 = [\n",
    "                        \"SizeOfStackReserve\",\\\n",
    "                        \"SizeOfStackCommit\",\\\n",
    "                        \"SizeOfHeapReserve\",\\\n",
    "                        \"SizeOfHeapCommit\",\\\n",
    "                        \"LoaderFlags\"]  # boolean check for zero or not\n",
    "    OPTIONAL_HEADER = OPTIONAL_HEADER1 + OPTIONAL_HEADER_DLL_char + OPTIONAL_HEADER2\n",
    "    Derived_header = [\"sus_sections\",\"non_sus_sections\", \"packer\",\"packer_type\",\"E_text\",\"E_data\",\"filesize\",\"E_file\",\"fileinfo\"]\n",
    "    \n",
    "    def __init__(self,source,output,label):\n",
    "        self.source = source\n",
    "        self.output = output\n",
    "        self.type = label\n",
    "\t#Need PEiD rules compile with yara\n",
    "        self.rules= yara.compile(filepath='./peid.yara')\n",
    "        \n",
    "    def file_creation_year(self,seconds):\n",
    "        tmp = 1970 + ((int(seconds) / 86400) / 365)\n",
    "        return int(tmp in range (1980,2016)) \n",
    "\n",
    "    def FILE_HEADER_Char_boolean_set(self,pe):\n",
    "        tmp = [pe.FILE_HEADER.IMAGE_FILE_RELOCS_STRIPPED,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_EXECUTABLE_IMAGE,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_LINE_NUMS_STRIPPED,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_LOCAL_SYMS_STRIPPED,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_AGGRESIVE_WS_TRIM,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_LARGE_ADDRESS_AWARE,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_BYTES_REVERSED_LO,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_32BIT_MACHINE,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_DEBUG_STRIPPED,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_REMOVABLE_RUN_FROM_SWAP,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_NET_RUN_FROM_SWAP,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_SYSTEM,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_DLL,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_UP_SYSTEM_ONLY,\\\n",
    "            pe.FILE_HEADER.IMAGE_FILE_BYTES_REVERSED_HI\n",
    "            ]\n",
    "        return [int(s) for s in tmp]\n",
    "\n",
    "    def OPTIONAL_HEADER_DLLChar(self,pe):\n",
    "        tmp = [\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_DYNAMIC_BASE,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_FORCE_INTEGRITY,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_NX_COMPAT ,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_NO_ISOLATION,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_NO_SEH,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_NO_BIND,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_WDM_DRIVER,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_TERMINAL_SERVER_AWARE,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_HIGH_ENTROPY_VA,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_APPCONTAINER,\\\n",
    "            pe.OPTIONAL_HEADER.IMAGE_DLLCHARACTERISTICS_GUARD_CF\n",
    "            ]\n",
    "        return [int(s) for s in tmp]\n",
    "\n",
    "    def Optional_header_ImageBase(self,ImageBase):\n",
    "        result= 0\n",
    "        if ImageBase % (64 * 1024) == 0 and ImageBase in [268435456,65536,4194304]:\n",
    "            result = 1\n",
    "        return result\n",
    "\n",
    "    def Optional_header_SectionAlignment(self,SectionAlignment,FileAlignment):\n",
    "        \"\"\"This is boolean function and will return 0 or 1 based on condidtions\n",
    "        that it SectionAlignment must be greater than or equal to FileAlignment\n",
    "        \"\"\"\n",
    "        return int(SectionAlignment >= FileAlignment)\n",
    "\n",
    "    def Optional_header_FileAlignment(self,SectionAlignment,FileAlignment):\n",
    "        result =0\n",
    "        if SectionAlignment >= 512:\n",
    "            if FileAlignment % 2 == 0 and FileAlignment in range(512,65537):\n",
    "                result =1\n",
    "        else: \n",
    "            if FileAlignment == SectionAlignment:\n",
    "                result = 1\n",
    "        return result\n",
    "\n",
    "    def Optional_header_SizeOfImage(self,SizeOfImage,SectionAlignment):\n",
    "\n",
    "        return int(SizeOfImage % SectionAlignment == 0)\n",
    "\n",
    "    def Optional_header_SizeOfHeaders(self,SizeOfHeaders,FileAlignment):\n",
    "\n",
    "        return int(SizeOfHeaders % FileAlignment == 0 )\n",
    "\n",
    "    def extract_dos_header(self,pe):\n",
    "        IMAGE_DOS_HEADER_data = [ 0 for i in range(6)]\n",
    "        try:\n",
    "            IMAGE_DOS_HEADER_data = [\n",
    "                                pe.DOS_HEADER.e_cblp,\\\n",
    "                                pe.DOS_HEADER.e_cp, \\\n",
    "                                pe.DOS_HEADER.e_cparhdr,\\\n",
    "                                pe.DOS_HEADER.e_maxalloc,\\\n",
    "                                pe.DOS_HEADER.e_sp,\\\n",
    "                                pe.DOS_HEADER.e_lfanew]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return IMAGE_DOS_HEADER_data\n",
    "\n",
    "    def extract_file_header(self,pe):\t\n",
    "        FILE_HEADER_data = [ 0 for i in range(3)]\n",
    "        FILE_HEADER_char =  []\n",
    "        try:\n",
    "            FILE_HEADER_data = [ \n",
    "                    pe.FILE_HEADER.NumberOfSections, \\\n",
    "                    self.file_creation_year(pe.FILE_HEADER.TimeDateStamp)]\n",
    "            FILE_HEADER_char = self.FILE_HEADER_Char_boolean_set(pe)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return FILE_HEADER_data + FILE_HEADER_char\n",
    "\n",
    "    def extract_optional_header(self,pe):\n",
    "        OPTIONAL_HEADER_data = [ 0 for i in range(21)]\n",
    "        DLL_char =[]\n",
    "        OPTIONAL_HEADER_data2 = [ 0 for i in range(6)]\n",
    "\n",
    "        try:\n",
    "            OPTIONAL_HEADER_data = [\n",
    "                pe.OPTIONAL_HEADER.MajorLinkerVersion,\\\n",
    "                pe.OPTIONAL_HEADER.MinorLinkerVersion,\\\n",
    "                pe.OPTIONAL_HEADER.SizeOfCode,\\\n",
    "                pe.OPTIONAL_HEADER.SizeOfInitializedData,\\\n",
    "                pe.OPTIONAL_HEADER.SizeOfUninitializedData,\\\n",
    "                pe.OPTIONAL_HEADER.AddressOfEntryPoint,\\\n",
    "                pe.OPTIONAL_HEADER.BaseOfCode,\\\n",
    "                pe.OPTIONAL_HEADER.BaseOfData,\\\n",
    "                #Check the ImageBase for the condition\n",
    "                self.Optional_header_ImageBase(pe.OPTIONAL_HEADER.ImageBase),\\\n",
    "                # Checking for SectionAlignment condition\n",
    "                self.Optional_header_SectionAlignment(pe.OPTIONAL_HEADER.SectionAlignment,pe.OPTIONAL_HEADER.FileAlignment),\\\n",
    "                #Checking for FileAlignment condition\n",
    "                self.Optional_header_FileAlignment(pe.OPTIONAL_HEADER.SectionAlignment,pe.OPTIONAL_HEADER.FileAlignment),\\\n",
    "                pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,\\\n",
    "                pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,\\\n",
    "                pe.OPTIONAL_HEADER.MajorImageVersion,\\\n",
    "                pe.OPTIONAL_HEADER.MinorImageVersion,\\\n",
    "                pe.OPTIONAL_HEADER.MajorSubsystemVersion,\\\n",
    "                pe.OPTIONAL_HEADER.MinorSubsystemVersion,\\\n",
    "                #Checking size of Image\n",
    "                self.Optional_header_SizeOfImage(pe.OPTIONAL_HEADER.SizeOfImage,pe.OPTIONAL_HEADER.SectionAlignment),\\\n",
    "                #Checking for size of headers\n",
    "                self.Optional_header_SizeOfHeaders(pe.OPTIONAL_HEADER.SizeOfHeaders,pe.OPTIONAL_HEADER.FileAlignment),\\\n",
    "                pe.OPTIONAL_HEADER.CheckSum,\\\n",
    "                pe.OPTIONAL_HEADER.Subsystem]\n",
    "\n",
    "            DLL_char = self.OPTIONAL_HEADER_DLLChar(pe)\n",
    "\n",
    "            OPTIONAL_HEADER_data2= [                \n",
    "                pe.OPTIONAL_HEADER.SizeOfStackReserve,\\\n",
    "                pe.OPTIONAL_HEADER.SizeOfStackCommit,\\\n",
    "                pe.OPTIONAL_HEADER.SizeOfHeapReserve,\\\n",
    "                pe.OPTIONAL_HEADER.SizeOfHeapCommit,\\\n",
    "                int(pe.OPTIONAL_HEADER.LoaderFlags == 0) ]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return OPTIONAL_HEADER_data + DLL_char + OPTIONAL_HEADER_data2\n",
    "\n",
    "    def get_count_suspicious_sections(self,pe):\n",
    "        result=[]\n",
    "        tmp =[]\n",
    "        benign_sections = set(['.text','.data','.rdata','.idata','.edata','.rsrc','.bss','.crt','.tls'])\n",
    "        for section in pe.sections:\n",
    "            tmp.append(section.Name.split('\\x00')[0])\n",
    "        non_sus_sections = len(set(tmp).intersection(benign_sections))\n",
    "        result=[len(tmp) - non_sus_sections, non_sus_sections]\n",
    "        return result\n",
    "\n",
    "    def check_packer(self,filepath):\n",
    "\n",
    "        result=[]\n",
    "        matches = self.rules.match(filepath)\n",
    "\n",
    "        try:\n",
    "            if matches == [] or matches == {}:\n",
    "                result.append([0,\"NoPacker\"])\n",
    "            else:\n",
    "                result.append([1,matches['main'][0]['rule']])\n",
    "        except:\n",
    "            result.append([1,matches[0]])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_text_data_entropy(self,pe):\n",
    "        result=[0.0,0.0]\n",
    "        for section in pe.sections:\n",
    "            s_name = section.Name.split('\\x00')[0]\n",
    "            if s_name == \".text\":\n",
    "                result[0]= section.get_entropy()\n",
    "            elif s_name == \".data\":\n",
    "                result[1]= section.get_entropy()\n",
    "            else:\n",
    "                pass\n",
    "        return result  \n",
    "\n",
    "    def get_file_bytes_size(self,filepath):\n",
    "        f = open(filepath, \"rb\")\n",
    "        byteArr = map(ord, f.read())\n",
    "        f.close()\n",
    "        fileSize = len(byteArr)\n",
    "        return byteArr,fileSize\n",
    "\n",
    "    def cal_byteFrequency(self,byteArr,fileSize):\n",
    "        freqList = []\n",
    "        for b in range(256):\n",
    "            ctr = 0\n",
    "            for byte in byteArr:\n",
    "                if byte == b:\n",
    "                    ctr += 1\n",
    "            freqList.append(float(ctr) / fileSize)\n",
    "        return freqList\n",
    "\n",
    "    def get_file_entropy(self,filepath):\n",
    "        byteArr, fileSize = self.get_file_bytes_size(filepath)\n",
    "        freqList = self.cal_byteFrequency(byteArr,fileSize)\n",
    "        # Shannon entropy\n",
    "        ent = 0.0\n",
    "        for freq in freqList:\n",
    "            if freq > 0:\n",
    "                ent +=  - freq * math.log(freq, 2)\n",
    "\n",
    "            #ent = -ent\n",
    "        return [fileSize,ent]\n",
    "\n",
    "    def get_fileinfo(self,pe):\n",
    "        result=[]\n",
    "        try:\n",
    "            FileVersion    = pe.FileInfo[0].StringTable[0].entries['FileVersion']\n",
    "            ProductVersion = pe.FileInfo[0].StringTable[0].entries['ProductVersion']\n",
    "            ProductName =    pe.FileInfo[0].StringTable[0].entries['ProductName']\n",
    "            CompanyName = pe.FileInfo[0].StringTable[0].entries['CompanyName']\n",
    "        #getting Lower and \n",
    "            FileVersionLS    = pe.VS_FIXEDFILEINFO.FileVersionLS\n",
    "            FileVersionMS    = pe.VS_FIXEDFILEINFO.FileVersionMS\n",
    "            ProductVersionLS = pe.VS_FIXEDFILEINFO.ProductVersionLS\n",
    "            ProductVersionMS = pe.VS_FIXEDFILEINFO.ProductVersionMS\n",
    "        except Exception as e:\n",
    "            result=[\"error\"]\n",
    "        #print \"{} while opening {}\".format(e,filepath)\n",
    "        else:\n",
    "        #shifting byte\n",
    "            FileVersion = (FileVersionMS >> 16, FileVersionMS & 0xFFFF, FileVersionLS >> 16, FileVersionLS & 0xFFFF)\n",
    "            ProductVersion = (ProductVersionMS >> 16, ProductVersionMS & 0xFFFF, ProductVersionLS >> 16, ProductVersionLS & 0xFFFF)\n",
    "            result = [FileVersion,ProductVersion,ProductName,CompanyName]\n",
    "        return int ( result[0] != 'error')\n",
    "\n",
    "    def write_csv_header(self):\n",
    "        filepath = self.output\n",
    "        HASH = ['filename', 'MD5']\n",
    "        header = HASH + self.IMAGE_DOS_HEADER + self.FILE_HEADER + self.OPTIONAL_HEADER + self.Derived_header\n",
    "        header.append(\"class\")\n",
    "        csv_file= open(filepath,\"w\")\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerow(header)\n",
    "        csv_file.close()\n",
    "\n",
    "    def extract_all(self,filepath):\n",
    "        data =[]\n",
    "        #load given file\n",
    "        try:\n",
    "            pe = pefile.PE(filepath)\n",
    "        except Exception as e:\n",
    "            print(\"{} while opening {}\".format(e,filepath))\n",
    "        else:\n",
    "            data += self.extract_dos_header(pe)\n",
    "            data += self.extract_file_header(pe)\n",
    "            data += self.extract_optional_header(pe)\n",
    "            # derived features\n",
    "            #number of suspicisou sections and non-suspicsious section\n",
    "            num_ss_nss = self.get_count_suspicious_sections(pe)\n",
    "            data += num_ss_nss\n",
    "            # check for packer and packer type\n",
    "            packer = self.check_packer(filepath)\n",
    "\n",
    "            # Appending the packer info to the rest of features\n",
    "            data += packer[0]\n",
    "            entropy_sections = self.get_text_data_entropy(pe)\n",
    "            data += entropy_sections\n",
    "            f_size_entropy = self.get_file_entropy(filepath)\n",
    "            data += f_size_entropy\n",
    "            fileinfo = self.get_fileinfo(pe)\n",
    "            data.append(fileinfo)\n",
    "            data.append(self.type)\n",
    "        \n",
    "        return data  \n",
    "\n",
    "    def write_csv_data(self,data):\n",
    "        filepath = self.output\n",
    "        csv_file= open(filepath,\"a\")\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerow(data)\n",
    "        csv_file.close()\n",
    "\n",
    "    def getMD5(self, filepath):\n",
    "        with open(filepath, 'rb') as fh:\n",
    "            m = hashlib.md5()\n",
    "            while True:\n",
    "                data = fh.read(8192)\n",
    "                if not data:\n",
    "                    break\n",
    "                m.update(data)\n",
    "            return m.hexdigest()\n",
    "\n",
    "        \n",
    "    def create_dataset(self):\n",
    "        self.write_csv_header()\n",
    "        count = 0\n",
    "\n",
    "        #run through all file of source and extract features\n",
    "        for file in os.listdir(self.source):         \n",
    "                filepath = self.source + file\n",
    "                data = self.extract_all(filepath)\n",
    "                hash_ = self.getMD5(filepath)\n",
    "                print (\"hash: \", hash_)\n",
    "                data.insert(0, hash_)\n",
    "                data.insert(0, file)\n",
    "\n",
    "                self.write_csv_data(data)\n",
    "                count += 1\n",
    "                print (\"Successfully Data extracted and written for {}.\".format(file))\n",
    "                print (\"Processed \" + str(count) + \" files\")\n",
    "\n",
    "def main():   \n",
    "\n",
    "    #샘플 파일이 위치하는 폴더\n",
    "    \n",
    "    source_path= 'E:/Heon/악성코드 탐지 모델/PJ1_malware/samples/normal/'\n",
    "#     raw_input(\"Enter the path of samples (ending with /) >>  \")\n",
    "    #특징 추출 결과를 저장할 csv파일 이름\n",
    "    output_file= 'E:/Heon/악성코드 탐지 모델/PJ1_malware/2-feature_eng/normal_pe.csv'\n",
    "#     raw_input(\"Give file name of output file. (.csv) >>\")\n",
    "    #label : 카테고리 지정, 악성코드인 경우1, 정상 프로그램은 0\n",
    "    label = 0,1\n",
    "#     raw_input(\"Enter type of sample( malware(1)|benign(0))>>\")\n",
    "\n",
    "    features = pe_features(source_path,output_file,label)    \n",
    "    features.create_dataset()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
